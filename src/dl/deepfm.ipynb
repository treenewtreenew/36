{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 以下是训练模型需要的参数\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM\n",
    "\n",
    "本文尝试用tensorflow高阶API实现DeepFM，从而让大家明白DeepFM的原理。\n",
    "\n",
    "DeepFM是深度CTR预估家族的一员，从名字上看似乎与FM有关，FM是浅层模型中CTR预估重要武器之一，这里DeepFM就是把Deep&Wide模型中LR部分换成了FM。\n",
    "\n",
    "所以DeepFM模型包含两个大模块：FM和DNN，而FM又分为LR和二阶特征隐因子组合。\n",
    "\n",
    "也就是一共三部分：\n",
    "\n",
    "1. LR: bias + wx\n",
    "2. FM: <v_i, v_j>x_i x_j\n",
    "3. DNN: deep \n",
    "\n",
    "输入的特征我们假设都是离散型的category特征，因为实际上工作遇到的场景也是以ID等离散型特征居多。\n",
    "\n",
    "本文一共分成三大部分：\n",
    "\n",
    "1. 处理输入特征，假设读入的数据是libsvm格式\n",
    "2. 构建DeepFM的计算图\n",
    "3. 保存模型，方便tensorflow serving加载\n",
    "\n",
    "下面进入第一部分。\n",
    "\n",
    "# 特征处理\n",
    "\n",
    "考虑到一般深度模型需要mini batch训练，因此，读入特征文件时，需要做打散和拆分。\n",
    "\n",
    "加入我们的特征文件存在文本中，可能文本存在本地或者集群HDFS上都行。而且假设特征文件是稀疏的libsvm格式。这时候需要用到tensorflow的data模块来处理。\n",
    "\n",
    "\n",
    "tf.data.TextLineDataset 可以读取文本行，专门用于存储稀疏特征样本的，比如文本，libsvm格式的样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TextLineDataset('../../data/criteo_conversion_logs/small_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一行是类似这样的数据：\n",
    "\n",
    "1 0:0:0.3651 2:1163:0.3651 3:8672:0.3651 4:2183:0.3651 5:2332:0.3651 6:185:0.3651 7:2569:0.3651 8:8131:0.3651 9:5483:0.3651 10:215:0.3651 11:1520:0.3651 12:1232:0.3651 13:2738:0.3651 14:2935:0.3651 15:5428:0.3651 17:2434:0.50000 16:7755:0.50000\n",
    "\n",
    "其中第一个是要预测的标签。\n",
    "\n",
    "后面的是空格分开的特征及特征值。\n",
    "\n",
    "每一个特征由三部分构成(冒号分开)：\n",
    "\n",
    "1. 字段ID\n",
    "2. 特征ID\n",
    "3. 特征值\n",
    "\n",
    "我们要用到TensorFlow自带的一些字符串工具来解析这个文本数据。解析后得到：\n",
    "\n",
    "1. 类别标签\n",
    "2. 特征的维度\n",
    "3. 特征向量（稀疏）\n",
    "\n",
    "tf.string_split可以用来做这件事，它的参数有：\n",
    "\n",
    "```\n",
    "tf.string_split(\n",
    "    source,\n",
    "    delimiter=' ',\n",
    "    skip_empty=True\n",
    ")\n",
    "```\n",
    "\n",
    "用delimiter作为拆分字符拆分source这个字段中的字符串。返回一个SparseTensor类型的实例。\n",
    "\n",
    "注意source字段保存的是一个形状为(1, D)的字符串Tensor实例，所以可以拆分多个字符串，如果只有一个字符串需要拆分，则要要以[string]这样的形式输入。\n",
    "\n",
    "假如输入两个字符串：['hello world', 'a b c']，返回则是这样一个SparseTensor：\n",
    "\n",
    "1. indices = [0, 0; 0, 1; 1, 0; 1, 1; 1, 2] \n",
    "2. shape = [2, 3]\n",
    "3. values = ['hello', 'world', 'a', 'b', 'c']\n",
    "\n",
    "也就是形如下面的稀疏张量：\n",
    "\n",
    "[\"hello\", \"world\"]\n",
    "\n",
    "[ \"a\",     \"b\",    \"c\"]\n",
    "\n",
    "是一个2 x 3的矩阵，只有(0,2)位置没有值，其他都有值，indices取值可以看出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_libsvm_to_tensor(line):\n",
    "    # 按照空格拆分\n",
    "    col = tf.string_split([line], ' ')\n",
    "    # 类别标签转成数值\n",
    "    label = tf.string_to_number(col.values[0], out_type=tf.float32)\n",
    "    # 字段、特征、值，三者一举拆分\n",
    "    ffv = tf.string_split(col.values[1:], ':')\n",
    "    has_field_id = True\n",
    "    if ffv.shape[1] == 2:\n",
    "        has_field_id = False\n",
    "    # ffv 是一个稠密矩阵，一共的行数是本样本中非零特征个数，列是3，第一列是字段ID，第二列是特征ID，第三列是取值\n",
    "    # 所以可以将ffv转换成一个稠密Tensor\n",
    "    ffv = tf.reshape(ffv.values, ffv.dense_shape)\n",
    "    # 分离ID和特征值\n",
    "    feature_ids = None\n",
    "    feature_vals = None\n",
    "    if has_field_id:\n",
    "        _, feature_ids, feature_vals = tf.split(ffv, num_or_size_splits=3, axis=1)\n",
    "    else:\n",
    "        feature_ids, feature_vals = tf.split(ffv, num_or_size_splits=2, axis=1)\n",
    "    feature_ids = tf.string_to_number(feature_ids, out_type=tf.int32)\n",
    "    feature_vals = tf.string_to_number(feature_vals, out_type=tf.float32)\n",
    "    return {\"feature_ids\": feature_ids, \"feature_vals\": feature_vals}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(transform_libsvm_to_tensor,num_parallel_calls=10).prefetch(200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "train_dataset = train_dataset.shuffle(buffer_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复迭代次数那么多次\n",
    "train_dataset = train_dataset.repeat(num_epochs)\n",
    "# 组成每次训练的批次\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "# Creates an Iterator for enumerating the elements of this dataset.\n",
    "#iterator = train_dataset.make_one_shot_iterator()\n",
    "#batch_features, batch_labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，第一部分完成。我们已经用tensorflow提供的接口把存储在文本中的libsvm格式数据准备成TensorFlow认识的格式。\n",
    "\n",
    "下面进行第二部分，构建模型。\n",
    "\n",
    "# 构建模型\n",
    "\n",
    "构建模型又分成三步工作：\n",
    "\n",
    "1. 网络的权重\n",
    "2. 输入的特征\n",
    "3. 网络计算图\n",
    "\n",
    "我们一步一步来构建。\n",
    "\n",
    "首先是网络权重，也是学习算法要去学习的，因此在训练过程中需要不断修改它，可变的量，要用一种特殊的张量保存：Variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(object):\n",
    "    \n",
    "    def __init__(self, params = {}):\n",
    "        self._field_size = params[\"field_size\"]\n",
    "        # 特征维度\n",
    "        self._feature_size = params[\"feature_size\"]\n",
    "        # FM的隐因子向量维度\n",
    "        self._embedding_size = params[\"embedding_size\"]\n",
    "        # L2正则参数\n",
    "        self._l2_reg = params[\"l2_reg\"]\n",
    "        # 学习率\n",
    "        self._learning_rate = params[\"learning_rate\"]\n",
    "        # DNN网络层数\n",
    "        self._layers  = map(int, params[\"deep_layers\"].split(','))\n",
    "        # dropout的概率\n",
    "        self._dropout = map(float, params[\"dropout\"].split(','))\n",
    "        # 执行模式\n",
    "        self._mode = params[\"mode\"]\n",
    "        # 批量归一化参数\n",
    "        self._batch_norm_decay = params['batch_norm_decay']\n",
    "    \n",
    "    def create_model(self):\n",
    "        # fm模型的偏置权重\n",
    "        self._fm_b = tf.get_variable(name='fm_b', shape=[1],\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "        # FM模型的一阶权重\n",
    "        self._fm_w = tf.get_variable(name='fm_w', shape=[self._feature_size],\n",
    "                               initializer=tf.glorot_normal_initializer())\n",
    "        # FM模型的二阶权重\n",
    "        self._fm_v = tf.get_variable(name='fm_v', shape=[self._feature_size, self._embedding_size],\n",
    "                               initializer=tf.glorot_normal_initializer())\n",
    "        \n",
    "    \n",
    "    # 前向计算\n",
    "    def forward(self, features, labels):\n",
    "        def batch_norm_layer(x, train_phase, scope_bn):\n",
    "            bn_train = tf.contrib.layers.batch_norm(x,\n",
    "                                                    decay=self._batch_norm_decay,\n",
    "                                                    center=True,\n",
    "                                                    scale=True,\n",
    "                                                    updates_collections=None,\n",
    "                                                    is_training=True,\n",
    "                                                    reuse=None,\n",
    "                                                    scope=scope_bn)\n",
    "            bn_infer = tf.contrib.layers.batch_norm(x,\n",
    "                                                    decay=self._batch_norm_decay,\n",
    "                                                    center=True,\n",
    "                                                    scale=True,\n",
    "                                                    updates_collections=None,\n",
    "                                                    is_training=False,\n",
    "                                                    reuse=True,\n",
    "                                                    scope=scope_bn)\n",
    "            z = tf.cond(tf.cast(train_phase, tf.bool),\n",
    "                        lambda: bn_train,\n",
    "                        lambda: bn_infer)\n",
    "            return z\n",
    "        \n",
    "        feature_ids  = features['feature_ids']\n",
    "        feature_ids = tf.reshape(feature_ids,shape=[-1,field_size])\n",
    "        feature_vals = features['feature_vals']\n",
    "        feature_vals = tf.reshape(feature_vals,shape=[-1,field_size])\n",
    "        \n",
    "        with tf.variable_scope(\"fm_wb\"):\n",
    "            # 线性模型一阶部分计算\n",
    "            feature_weights = tf.nn.embedding_lookup(self._fm_w, feature_ids)    \n",
    "            y_w = tf.reduce_sum(tf.multiply(feature_weights, feature_vals),1)\n",
    "\n",
    "        with tf.variable_scope(\"fm_vv\"):\n",
    "            # 特征的隐因子向量, 论文中的K维\n",
    "            embeddings = tf.nn.embedding_lookup(self._fm_v, feature_ids)\n",
    "            # 这里是按照FM论文中变形后的公式计算以提高计算效率\n",
    "            feature_vals = tf.reshape(feature_vals, shape=[-1, field_size, 1])\n",
    "            embeddings = tf.multiply(embeddings, feature_vals)                 \n",
    "            sum_square = tf.square(tf.reduce_sum(embeddings,1))\n",
    "            square_sum = tf.reduce_sum(tf.square(embeddings),1)\n",
    "            y_v = 0.5 * tf.reduce_sum(tf.subtract(sum_square, square_sum),1)  \n",
    "\n",
    "        train_phase = False\n",
    "        if self._mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_phase = True\n",
    "\n",
    "        l2_reg = tf.contrib.layers.l2_regularizer(self._l2_reg)\n",
    "        with tf.variable_scope(\"dnn\"):\n",
    "            # 深度模型部分，输入为FM的隐因子向量\n",
    "            deep_inputs = tf.reshape(embeddings, \n",
    "                                     shape=[-1,field_size*embedding_size])  \n",
    "            for i in range(len(self._layers)):\n",
    "                deep_inputs = tf.contrib.layers.fully_connected(inputs=deep_inputs,\n",
    "                                                                num_outputs=layers[i], \n",
    "                                                                weights_regularizer=l2_reg,\n",
    "                                                                scope='dnn%d' % i)\n",
    "                if self._batch_norm:\n",
    "                    deep_inputs = batch_norm_layer(deep_inputs,\n",
    "                                               train_phase=train_phase,\n",
    "                                               scope_bn='bn_%d' %i)\n",
    "                if train_phase:\n",
    "                    deep_inputs = tf.nn.dropout(deep_inputs, keep_prob=self._dropout[i])\n",
    "        \n",
    "            y_d = tf.contrib.layers.fully_connected(inputs=deep_inputs,\n",
    "                                                   num_outputs=1,\n",
    "                                                   activation_fn=tf.identity, \n",
    "                                                   weights_regularizer=l2_reg,\n",
    "                                                   scope='dout')\n",
    "            y_d = tf.reshape(y_d,shape=[-1])\n",
    "        \n",
    "        with tf.variable_scope(\"out\"):\n",
    "            y_bias = self._fm_b * tf.ones_like(y_d, dtype=tf.float32)      # None * 1\n",
    "            y = y_bias + y_w + y_v + y_d\n",
    "            prediction = tf.sigmoid(y)\n",
    "\n",
    "        predictions={\"prediction\": prediction}\n",
    "         # 批量预测模式则导出预测结果\n",
    "        if self._mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "                              tf.estimator.export.PredictOutput(predictions)}\n",
    "            return tf.estimator.EstimatorSpec(mode=self._mode,\n",
    "                                              predictions=predictions,\n",
    "                                              export_outputs=export_outputs)       \n",
    "        \n",
    "        # 无论是训练模式还是评价模式，都需要计算损失\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels))  \\\n",
    "                                + l2_reg * tf.nn.l2_loss(self._fm_w)  \\\n",
    "                                + l2_reg * tf.nn.l2_loss(self._fm_v)\n",
    "\n",
    "        # 模型评估模式则计算AUC\n",
    "        if self._mode == tf.estimator.ModeKeys.EVAL:\n",
    "            eval_metric_ops = {\n",
    "                \"auc\": tf.metrics.auc(labels, prediction)\n",
    "            }\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(mode=self._mode,\n",
    "                                              predictions=predictions,\n",
    "                                              loss=loss,\n",
    "                                              eval_metric_ops=eval_metric_ops)        \n",
    "        # 训练模型则计算损失\n",
    "        if self._mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self._learning_rate,\n",
    "                                               beta1=0.832,\n",
    "                                               beta2=0.987,\n",
    "                                               epsilon=1e-8)\n",
    "            train_operator = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(mode=self._mode,\n",
    "                                              predictions=predictions,\n",
    "                                              loss=loss,\n",
    "                                              train_op=train_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
